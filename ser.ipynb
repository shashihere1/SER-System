import os
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np

# Path to the ALL directory containing the audio files
ravdess = "/kaggle/input/saveee/archive (1)/ALL/"
audio_files = os.listdir(ravdess)

# Define a dictionary for emotion mapping (based on SAVEE file naming convention)
emotion_mapping = {
    'a': 'Angry',
    'd': 'Disgust',
    'f': 'Fear',
    'h': 'Happy',
    'n': 'Neutral',
    'sa': 'Sad',
    'su': 'Surprise'
}

# Function to extract emotion from filename
def extract_emotion(filename):
    emotion_code = filename.split('_')[1][:2]  # Extract the emotion code (e.g., 'a', 'd', 'sa')
    return emotion_mapping.get(emotion_code, 'Unknown')  # Map to emotion label

# Initialize lists to store emotions and file paths
file_emotions = []
file_paths = []

# Process each audio file
for file in audio_files:
    full_path = os.path.join(ravdess, file)
    if os.path.isfile(full_path):
        emotion = extract_emotion(file)  # Extract emotion
        file_emotions.append(emotion)
        file_paths.append(full_path)

# Display the collected data
print(f"Total audio files processed: {len(file_paths)}")
print(f"Emotions extracted: {set(file_emotions)}")

# Function to plot waveplot and spectrogram for an audio file
def plot_audio_features(file_path, emotion):
    plt.figure(figsize=(14, 8))
    
    # Load the audio file
    y, sr = librosa.load(file_path)
    
    # Plot Waveplot
    plt.subplot(2, 1, 1)
    librosa.display.waveshow(y, sr=sr, alpha=0.7)
    plt.title(f"Waveplot - {emotion}")
    plt.xlabel("Time (s)")
    plt.ylabel("Amplitude")
    
    # Plot Spectrogram
    plt.subplot(2, 1, 2)
    spectrogram = librosa.stft(y)
    spectrogram_db = librosa.amplitude_to_db(abs(spectrogram))
    librosa.display.specshow(spectrogram_db, sr=sr, x_axis='time', y_axis='hz', cmap='viridis')
    plt.colorbar(format="%+2.0f dB")
    plt.title(f"Spectrogram - {emotion}")
    plt.xlabel("Time (s)")
    plt.ylabel("Frequency (Hz)")
    
    plt.tight_layout()
    plt.show()

# Plot waveplot and spectrogram for a few sample files
sample_files = 5
for i in range(sample_files):
    plot_audio_features(file_paths[i], file_emotions[i])

# Function to extract features from audio (MFCC)
def extract_features(file_path):
    y, sr = librosa.load(file_path)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
    mfcc_scaled = np.mean(mfcc.T, axis=0)
    return mfcc_scaled

# Prepare the dataset for training (features and labels)
features = []
labels = []

for file_path, emotion in zip(file_paths, file_emotions):
    feature = extract_features(file_path)
    features.append(feature)
    labels.append(emotion)

# Convert to numpy arrays
features = np.array(features)
labels = np.array(labels)

# Encode labels
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
encoded_labels = encoder.fit_transform(labels)

# Split the dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(features, encoded_labels, test_size=0.2, random_state=42)

# Train a classifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Initialize and train the model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Test the model
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

# Display results
print(f"Model Accuracy: {accuracy * 100:.2f}%")
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=encoder.classes_))

# Save the trained model
import joblib
joblib.dump(model, "speech_emotion_model.pkl")
